{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Image_Classification_TF2.ipynb","provenance":[{"file_id":"15QZMSIrvE4MgVq2GnQ0XUsqQssY6sn7w","timestamp":1572616939723}],"private_outputs":true,"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"8T1Pf4mbuVRI","colab_type":"text"},"source":["\n","#Image Classification From Scratch  With TensorFlow 2.0\n","\n","In this notebook, we'll be training a Convolutional Neural Network ( CNN ) without using Keras, all with raw TF 2.0 APIs!\n","\n","We'll declare weights, loss function and a optimizer for our CNN and train it on the Horses-Or-Humans dataset available on TensorFlow Datasets\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UcqgU2fXu-Yw","colab_type":"text"},"source":["\n","## 1) Installing TensorFlow 2.0\n","\n","First, if our Colab's default TensorFlow version is not 2.0 then we need to upgrade the TF package using `pip`.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"vkcq3St-gZsd","colab_type":"code","colab":{}},"source":["\n","!pip install --upgrade tensorflow\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BOhEmurMvlO9","colab_type":"text"},"source":["\n","## 2) Importing packages and declaring useful variables\n","\n","We declare some useful variables like `batch_size`, `padding` for convolutional layers, the `learning_rate` and so on.\n"]},{"cell_type":"code","metadata":{"id":"QYda818kgpz8","colab_type":"code","colab":{}},"source":["\n","import tensorflow as tf\n","import tensorflow_datasets as tfds\n","\n","padding = \"SAME\"  #@param ['SAME', 'VALID' ]\n","num_output_classes = 102  #@param {type: \"number\"}\n","batch_size = 32  #@param {type: \"number\"}\n","learning_rate = 0.001  #@param {type: \"number\"}\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hjsIAAxzxoii","colab_type":"text"},"source":["\n","## 3) Fetching the Dataset\n","\n","We fetch our dataset from TensorFlow Datasets which makes much of the data preprocessing easier ;-)\n"]},{"cell_type":"code","metadata":{"id":"XglEBv0KjNF0","colab_type":"code","colab":{}},"source":["\n","dataset_name = 'horses_or_humans'  #@param {type: \"string\"}\n","\n","dataset = tfds.load( name=dataset_name , split=tfds.Split.TRAIN )\n","dataset = dataset.shuffle( 1024 ).batch( batch_size )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sZ3W9NCx8RB","colab_type":"text"},"source":["\n","## 4) Defining CNN operations\n","\n","For our CNN model, we mainly use three operations ( layers ).\n","\n","1. `conv2d` : Performs convolutions over the `inputs` matrix with kernels ( `filters` ) and `stride_size`. Also, it performs the Leaky ReLU activation function.\n","\n","2. `maxpool` : Performs max pooling over the `inputs`.\n","\n","3. `dense` : Dense layers for the CNN with dropout.\n"]},{"cell_type":"code","metadata":{"id":"oUwYW2IXg8hl","colab_type":"code","colab":{}},"source":["\n","leaky_relu_alpha = 0.2 #@param {type: \"number\"}\n","dropout_rate = 0.5 #@param {type: \"number\"}\n","\n","def conv2d( inputs , filters , stride_size ):\n","    out = tf.nn.conv2d( inputs , filters , strides=[ 1 , stride_size , stride_size , 1 ] , padding=padding ) \n","    return tf.nn.leaky_relu( out , alpha=leaky_relu_alpha ) \n","\n","def maxpool( inputs , pool_size , stride_size ):\n","    return tf.nn.max_pool2d( inputs , ksize=[ 1 , pool_size , pool_size , 1 ] , padding='VALID' , strides=[ 1 , stride_size , stride_size , 1 ] )\n","\n","def dense( inputs , weights ):\n","    x = tf.nn.leaky_relu( tf.matmul( inputs , weights ) , alpha=leaky_relu_alpha )\n","    return tf.nn.dropout( x , rate=dropout_rate )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7XNdAsWky7eg","colab_type":"text"},"source":["\n","## 5) Initializing CNN weights\n","\n","We initialize the weights for our CNN. The shapes need to calculated but the `tf.nn.conv2d` expects the filters to have a shape of `[ kernel_size , kernel_size , in_dims , out_dims ]`.\n","\n","We use the `glorot_uniform` initializer for our weights.\n"]},{"cell_type":"code","metadata":{"id":"VXSI7DnehJhE","colab_type":"code","colab":{}},"source":["\n","output_classes = 3\n","initializer = tf.initializers.glorot_uniform()\n","def get_weight( shape , name ):\n","    return tf.Variable( initializer( shape ) , name=name , trainable=True , dtype=tf.float32 )\n","\n","shapes = [\n","    [ 3 , 3 , 3 , 16 ] , \n","    [ 3 , 3 , 16 , 16 ] , \n","    [ 3 , 3 , 16 , 32 ] , \n","    [ 3 , 3 , 32 , 32 ] ,\n","    [ 3 , 3 , 32 , 64 ] , \n","    [ 3 , 3 , 64 , 64 ] ,\n","    [ 3 , 3 , 64 , 128 ] , \n","    [ 3 , 3 , 128 , 128 ] ,\n","    [ 3 , 3 , 128 , 256 ] , \n","    [ 3 , 3 , 256 , 256 ] ,\n","    [ 3 , 3 , 256 , 512 ] , \n","    [ 3 , 3 , 512 , 512 ] ,\n","    [ 8192 , 3600 ] , \n","    [ 3600 , 2400 ] ,\n","    [ 2400 , 1600 ] , \n","    [ 1600 , 800 ] ,\n","    [ 800 , 64 ] ,\n","    [ 64 , output_classes ] ,\n","]\n","\n","weights = []\n","for i in range( len( shapes ) ):\n","    weights.append( get_weight( shapes[ i ] , 'weight{}'.format( i ) ) )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vsCz6ONvziKX","colab_type":"text"},"source":["\n","## 6) Assembling the operations\n","\n","We put together all the CNN ops we defined earlier into a final model which we will use for training finally.\n"]},{"cell_type":"code","metadata":{"id":"Py8Ggzs2hPqO","colab_type":"code","colab":{}},"source":["\n","def model( x ) :\n","    x = tf.cast( x , dtype=tf.float32 )\n","    c1 = conv2d( x , weights[ 0 ] , stride_size=1 ) \n","    c1 = conv2d( c1 , weights[ 1 ] , stride_size=1 ) \n","    p1 = maxpool( c1 , pool_size=2 , stride_size=2 )\n","    \n","    c2 = conv2d( p1 , weights[ 2 ] , stride_size=1 )\n","    c2 = conv2d( c2 , weights[ 3 ] , stride_size=1 ) \n","    p2 = maxpool( c2 , pool_size=2 , stride_size=2 )\n","    \n","    c3 = conv2d( p2 , weights[ 4 ] , stride_size=1 ) \n","    c3 = conv2d( c3 , weights[ 5 ] , stride_size=1 ) \n","    p3 = maxpool( c3 , pool_size=2 , stride_size=2 )\n","    \n","    c4 = conv2d( p3 , weights[ 6 ] , stride_size=1 )\n","    c4 = conv2d( c4 , weights[ 7 ] , stride_size=1 )\n","    p4 = maxpool( c4 , pool_size=2 , stride_size=2 )\n","\n","    c5 = conv2d( p4 , weights[ 8 ] , stride_size=1 )\n","    c5 = conv2d( c5 , weights[ 9 ] , stride_size=1 )\n","    p5 = maxpool( c5 , pool_size=2 , stride_size=2 )\n","\n","    c6 = conv2d( p5 , weights[ 10 ] , stride_size=1 )\n","    c6 = conv2d( c6 , weights[ 11 ] , stride_size=1 )\n","    p6 = maxpool( c6 , pool_size=2 , stride_size=2 )\n","\n","    flatten = tf.reshape( p6 , shape=( tf.shape( p6 )[0] , -1 ))\n","\n","    d1 = dense( flatten , weights[ 12 ] )\n","    d2 = dense( d1 , weights[ 13 ] )\n","    d3 = dense( d2 , weights[ 14 ] )\n","    d4 = dense( d3 , weights[ 15 ] )\n","    d5 = dense( d4 , weights[ 16 ] )\n","    logits = tf.matmul( d5 , weights[ 17 ] )\n","\n","    return tf.nn.softmax( logits )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_P-L8iQWz29J","colab_type":"text"},"source":["\n","## 7) Defining the loss function and optimization using `tf.GradientTape`\n","\n","We use `tf.losses.categorical_crossentropy` as our loss function. Now comes `tf.GradientTape`, an automatic differentiation engine which records all the derivatives within its scope.\n","\n","The `train_step` function takes in a batch of data, calculates the loss and gradients and then with `optimizer.apply_gradients`, we update the `weights`.\n"]},{"cell_type":"code","metadata":{"id":"U61C5trri_xu","colab_type":"code","colab":{}},"source":["\n","def loss( pred , target ):\n","    return tf.losses.categorical_crossentropy( target , pred )\n","\n","optimizer = tf.optimizers.Adam( learning_rate )\n","\n","def train_step( model, inputs , outputs ):\n","    with tf.GradientTape() as tape:\n","        current_loss = loss( model( inputs ), outputs)\n","    grads = tape.gradient( current_loss , weights )\n","    optimizer.apply_gradients( zip( grads , weights ) )\n","    print( tf.reduce_mean( current_loss ) )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k8CUgMw70V9P","colab_type":"text"},"source":["\n","## 8) Final Training\n","\n","We train our model for a specific number of epochs.\n"]},{"cell_type":"code","metadata":{"id":"DL42kBKgk2U2","colab_type":"code","colab":{}},"source":["\n","num_epochs = 256 #@param {type: \"number\"}\n","\n","for e in range( num_epochs ):\n","    for features in dataset:\n","        image , label = features[ 'image' ] , features[ 'label' ]\n","        train_step( model , image , tf.one_hot( label , depth=3 ) )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QSdM0ti0l-lO","colab_type":"text"},"source":["\n","That's All. You can now export the graph or play around with more layers or fine-tuning the hyperparameters.\n"]}]}