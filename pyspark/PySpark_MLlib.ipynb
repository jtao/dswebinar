{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jtao/dswebinar/blob/master/pyspark/PySpark_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZxC8jYc6HHT"
   },
   "source": [
    "# PySpark DataFrames and SQL\n",
    "\n",
    "[Jian Tao](https://tx.ag/jtao), Texas A&M University\n",
    "\n",
    "June 30, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPELjsLYhub-"
   },
   "source": [
    "### 1. Set up the PySpark environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJlWGm4aEZeM",
    "outputId": "a9375357-d814-43b8-b998-4dca175ea9b2"
   },
   "outputs": [],
   "source": [
    "# For each Google Colab, we will need to run this cell to ensure that PySpark is installed properly.\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "  !pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").config('spark.ui.port', '4050').getOrCreate()\n",
    "spark\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip -o ngrok-stable-linux-amd64.zip\n",
    "# get_ipython().system_raw('./ngrok http 4050 &')\n",
    "# !curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(\\\"\\nClick me to launch (give it a minute or two)\\n\\\"); print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQhDvdKrhucB"
   },
   "source": [
    "### 2. Create a DataFrame by reading from a CSV/JSON file\n",
    "\n",
    "`spark.read.csv` can only read from local files, so we will have to download the CSV file from the URL first. We can use `SparkFiles` to do that or use `pandas`. For those CSV files with a header, please make sure to set `header=True` in the argument list for `spark.read.csv`. When the data types of the columns are not known, `inferSchema=True` will do the trick to automatically recognize the data types, but it is not perfect. In our example, `Horsepower` is not correctly recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VhDbsJr64Xh",
    "outputId": "0d7869e4-9d5d-4843-ccf9-6306aa585eca"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "if IN_COLAB:\n",
    "  csv_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.csv\"\n",
    "  json_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.json\"\n",
    "else:\n",
    "  csv_url = \"Auto.csv\"  \n",
    "  json_url = \"Auto.json\"\n",
    "spark.sparkContext.addFile(csv_url)\n",
    "spark.sparkContext.addFile(json_url)\n",
    "\n",
    "## One can create a spark dataframe from pandas dataframe as well.\n",
    "# import pandas as pd\n",
    "# df = spark.createDataFrame(pd.read_csv(url))\n",
    "\n",
    "#df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=False)\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD68fXq5hucF"
   },
   "source": [
    "### 3. Create a Linear Regression Model with MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXc67xgGlUPF"
   },
   "source": [
    "First, we will need to split the dataset into training (70%) and testing (30%) datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UpVtzFOhucG"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "vectorAssembler = VectorAssembler(inputCols = ['weight', 'displacement', 'acceleration', 'cylinders'], outputCol = 'features')\n",
    "df = vectorAssembler.transform(df)\n",
    "df = df.select(['features', 'mpg'])\n",
    "splits = df.randomSplit([0.7, 0.3])\n",
    "\n",
    "train_df = splits[0]\n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ln3c7ETxlhNP",
    "outputId": "b222937c-fe31-472c-d1a1-d61bb11cb0af"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='mpg', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sNKN8YQhpeEy",
    "outputId": "413b977c-66fd-49a1-a186-ec7d54a852bb"
   },
   "outputs": [],
   "source": [
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3H6hr8Tz0yf6",
    "outputId": "90bc3986-bd39-4160-f443-84a040d868c5"
   },
   "outputs": [],
   "source": [
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9oD5tKs1kIR"
   },
   "source": [
    "Check the resutls with the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s8TXiCi-1Mn7",
    "outputId": "01a1c6d5-bccc-4720-bfd4-2e30baa8f0a0"
   },
   "outputs": [],
   "source": [
    "lr_predictions = lr_model.transform(test_df)\n",
    "lr_predictions.select(\"prediction\",\"mpg\",\"features\").show(5)\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"mpg\",metricName=\"r2\")\n",
    "print(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Z_EqJ3bhucJ"
   },
   "source": [
    "### 4. References:\n",
    "\n",
    "SQL References\n",
    "https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "PySpark MLlib.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
