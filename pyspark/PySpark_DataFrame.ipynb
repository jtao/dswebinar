{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jtao/dswebinar/blob/master/pyspark/PySpark_DataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZxC8jYc6HHT"
   },
   "source": [
    "# PySpark DataFrames and SQL\n",
    "\n",
    "[Jian Tao](https://tx.ag/jtao), Texas A&M University\n",
    "\n",
    "June 30, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa-YWlj2p2dV"
   },
   "source": [
    "### 1. Set up the PySpark environment first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJlWGm4aEZeM",
    "outputId": "5ae8568c-c868-4558-9a6d-3aa20feb34de"
   },
   "outputs": [],
   "source": [
    "# For each Google Colab, we will need to run this cell to ensure that PySpark is installed properly.\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "  !pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Test\").config('spark.ui.port', '4050').getOrCreate()\n",
    "spark\n",
    "# !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "# !unzip -o ngrok-stable-linux-amd64.zip\n",
    "# get_ipython().system_raw('./ngrok http 4050 &')\n",
    "# !curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(\\\"\\nClick me to launch (give it a minute or two)\\n\\\"); print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_4Zin3ep2dX"
   },
   "source": [
    "### 2. Create a DataFrame by reading from a CSV/JSON file\n",
    "\n",
    "`spark.read.csv` can only read from local files, so we will have to download the CSV file from the URL first. We can use `SparkFiles` to do that or use `pandas`. For those CSV files with a header, please make sure to set `header=True` in the argument list for `spark.read.csv`. When the data types of the columns are not known, `inferSchema=True` will do the trick to automatically recognize the data types, but it is not perfect. In our example, `Horsepower` is not correctly recognized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8VhDbsJr64Xh",
    "outputId": "1fe27497-5c1f-42c5-9216-f2e02163ab29"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "if IN_COLAB:\n",
    "  csv_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.csv\"\n",
    "  json_url = \"https://raw.githubusercontent.com/jtao/dswebinar/master/pyspark/Auto.json\"\n",
    "else:\n",
    "  csv_url = \"Auto.csv\"  \n",
    "  json_url = \"Auto.json\"\n",
    "spark.sparkContext.addFile(csv_url)\n",
    "spark.sparkContext.addFile(json_url)\n",
    "\n",
    "## One can create a spark dataframe from pandas dataframe as well.\n",
    "# import pandas as pd\n",
    "# df = spark.createDataFrame(pd.read_csv(url))\n",
    "\n",
    "#df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=False)\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSes5lzyp2dY",
    "outputId": "e909d885-d186-4611-af9f-8c9761cfb831"
   },
   "outputs": [],
   "source": [
    "# either will work\n",
    "df = spark.read.json(SparkFiles.get(\"Auto.json\"))\n",
    "#df = spark.read.load(SparkFiles.get(\"Auto.json\"),format=\"json\")\n",
    "\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZnvcbVS0p2dY"
   },
   "source": [
    "We can define a schema to help `spark.read.csv` to correctly cast the type of all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SLJ4ZWApDCbS",
    "outputId": "b90ddb3e-08d7-4358-e928-85eccac7d34a"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "user_schema = StructType([\n",
    "                     StructField(\"mpg\", IntegerType(), True),\n",
    "                     StructField(\"cylinders\", IntegerType(), True),\n",
    "                     StructField(\"displacement\", IntegerType(), True),\n",
    "                     StructField(\"horsepower\", IntegerType(), True),\n",
    "                     StructField(\"weight\", IntegerType(), True),\n",
    "                     StructField(\"acceleration\", DoubleType(), True),\n",
    "                     StructField(\"year\", IntegerType(), True),\n",
    "                     StructField(\"origin\", IntegerType(), True),\n",
    "                     StructField(\"name\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", schema=user_schema, inferSchema=True)\n",
    "\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oy3fb1Zzp2dZ",
    "outputId": "098407dd-80b6-42c2-9c10-32136859d583"
   },
   "outputs": [],
   "source": [
    "df = spark.read.json(SparkFiles.get(\"Auto.json\"))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PPLj5hPp2da"
   },
   "source": [
    "### 3. Create a Spark DataFrame with a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "schVRvA8p2da",
    "outputId": "3a21659f-c530-4351-8ff3-539b9a3a82dc"
   },
   "outputs": [],
   "source": [
    "auto_list = [(1, 18, \"Chevrolet\"), (2, 15, \"Buick\"), (3, 18, \"Plymouth\"), (4, 16, \"Amc\"), (5, 17, \"Ford\")]\n",
    "\n",
    "df = spark.createDataFrame(auto_list)\n",
    "df.printSchema()\n",
    "df.show(5)\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "users_schema = StructType([\n",
    "                          StructField(\"id\", IntegerType(), True),\n",
    "                          StructField(\"mpg\", IntegerType(), True),\n",
    "                          StructField(\"name\", StringType(), True)])\n",
    "\n",
    "df = spark.createDataFrame(auto_list, schema=users_schema)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lATQPZOip2db"
   },
   "source": [
    "### 4. Create a Spark DataFrame with a list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yj3_hRdEp2db",
    "outputId": "dfbf6767-4e7b-42f7-f74a-0bc6d6837485"
   },
   "outputs": [],
   "source": [
    "auto_list = [{\"id\": 1, \"mpg\": 18, \"name\": \"Chevrolet\"}, \n",
    "                {\"id\": 2, \"mpg\": 15, \"name\": \"Buick\"}, \n",
    "                {\"id\": 3, \"mpg\": 18, \"name\": \"Plymouth\"}, \n",
    "                {\"id\": 4, \"mpg\": 16, \"name\": \"Amc\"}, \n",
    "                {\"id\": 5, \"mpg\": 17, \"name\": \"Ford\"}]\n",
    "df = spark.createDataFrame(auto_list)\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gse5uviKp2dc"
   },
   "source": [
    "### 5. Operations on Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAGX32_zp2dc"
   },
   "outputs": [],
   "source": [
    "# Load the full data set again.\n",
    "df = spark.read.csv(SparkFiles.get(\"Auto.csv\"), header=True, sep=\",\", inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ICgVhl2p2dc",
    "outputId": "b42c6bbd-e14a-488d-d81b-1e295fdbfcff"
   },
   "outputs": [],
   "source": [
    "# Select only the \"name\" column\n",
    "df.select(\"name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtqxjtPIp2dc",
    "outputId": "7e380b4e-0da4-448f-f241-6a3ed31e345f"
   },
   "outputs": [],
   "source": [
    "# Select everybody, but increment the mpg by 100\n",
    "df.select(df['name'], df['mpg'] + 100).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "chnSjcLqp2dd",
    "outputId": "4df9f4c5-f869-4226-83a6-5eb2128cdc23"
   },
   "outputs": [],
   "source": [
    "# Select mpg greater than 30\n",
    "df.filter(df['mpg'] > 30).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1D5VACO8p2dd",
    "outputId": "b6648480-21b3-4c25-f2ab-257c73ae005c"
   },
   "outputs": [],
   "source": [
    "# Count Cars by cylinders\n",
    "df.groupBy(\"cylinders\").count().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KWmH_4gp2dd"
   },
   "source": [
    "### 6. Running SQL queries programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NKDKrHNap2dd",
    "outputId": "e10ea611-1b2f-465d-8715-3bdcc7824821"
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view\n",
    "df.createOrReplaceTempView(\"auto\")\n",
    "sqlDF = spark.sql(\"SELECT * FROM auto\")\n",
    "sqlDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNjfJ89np2de"
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a global temporary view\n",
    "df.createGlobalTempView(\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8K71_2-p2de",
    "outputId": "a2cb182d-04e0-406f-fd63-d25c8adfd072"
   },
   "outputs": [],
   "source": [
    "# Global temporary view is tied to a system preserved database `global_temp`\n",
    "spark.sql(\"SELECT * FROM global_temp.auto\").show(5)\n",
    "\n",
    "# Global temporary view is cross-session\n",
    "spark.newSession().sql(\"SELECT * FROM global_temp.auto\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SISV_tNMp2de"
   },
   "source": [
    "### 7. References:\n",
    "\n",
    "SQL References\n",
    "https://spark.apache.org/docs/latest/sql-ref-ansi-compliance.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Create DataFrame from CSV File in PySpark 3.0 on Google Colab | Part 3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
